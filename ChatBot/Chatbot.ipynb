{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQYZpTuDdR5884idcE01O6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atulkhiyani0909/NyayaSetu/blob/main/ChatBot/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "TvQjPnDAg68p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LBi0SE2NInv9"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain-core langchain langchain-google-genai pinecone-client langchain-pinecone langchain-huggingface pydantic gradio transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "1XOfUpvMhl4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "jzZ95U33g-Pm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ['PINECONE_API_KEY']=userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "FfFKk6q7iDcq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "f-Xvdh2MkHrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict\n",
        "\n",
        "# Core ML and RAG Libraries\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from pinecone import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from pydantic import BaseModel,Field\n",
        "from functools import lru_cache\n",
        "from langchain.load import dumps,loads"
      ],
      "metadata": {
        "id": "ThN-H1Wsib1b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(maxsize=1)\n",
        "def get_llm(temp=0):\n",
        "    return ChatGoogleGenerativeAI(model='gemini-2.0-flash',temperature=temp)\n",
        "\n",
        "llm = get_llm()\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class language_detector(BaseModel):\n",
        "    language: str = Field(..., description=\"Detected Language\")\n",
        "    translated: str = Field(..., description=\"Translated to English\")\n",
        "\n",
        "def query_to_english(query:str,memory) -> dict:\n",
        "    \"\"\"Detects the language of the input query and translates it to English.\"\"\"\n",
        "\n",
        "    lan_example = '''{\n",
        "        \"language\": \"Hindi\",\n",
        "        \"translated\": \"Hello, how are you?\"\n",
        "    }'''\n",
        "\n",
        "    prompt = \"\"\"Translate the following query to clear English while preserving its context and intent you may imporve its wordings for better understanding.\n",
        "    If the query is ambiguous, you can rephrase it, but do not change its original meaning utilize this Chat history to rewrite\n",
        "    this ambigous query : {memory}\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Also, detect the language of the query and store it in \"language\".\n",
        "\n",
        "    Output should strictly follow this format:\n",
        "    {example}\n",
        "    \"\"\"\n",
        "\n",
        "    llm3 = get_llm().with_structured_output(language_detector)\n",
        "\n",
        "    trans_template = ChatPromptTemplate.from_template(\n",
        "        template=prompt,\n",
        "        partial_variables={\n",
        "            'example': lan_example,\n",
        "            'memory': memory\n",
        "            }\n",
        "    )\n",
        "\n",
        "    trans_chain = trans_template | llm3  # No need for StrOutputParser since output is structured\n",
        "    return trans_chain.invoke({'query': query}).model_dump()  # Ensure structured dict output"
      ],
      "metadata": {
        "id": "NWUcVbuLkwkI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TalkBack(BaseModel):\n",
        "    talkback: bool = Field(..., description=\"Talkback\")\n",
        "\n",
        "#This decide should it talkback or should go for direct retrieval which ia a little time consuming!\n",
        "def should_talkback(query: str,memory) -> dict:\n",
        "    \"\"\"Should talkback or not\"\"\"\n",
        "\n",
        "    prompt = '''\n",
        "As a legal assistant for NyayaSetu,utilize memory for deciding -- only return `True` if:\n",
        "    1. The query is impossible to answer without more details (e.g., \"What happens?\" without context)\n",
        "    2. Combines 3+ unrelated legal issues\n",
        "    3. Contains contradictory information\n",
        "    4. Contains greeting or useless information or talk (cross check using chat_history)\n",
        "\n",
        "    Return `False` if:\n",
        "    1. Query describes a single clear legal issue\n",
        "    2. User has provided sufficient context\n",
        "    3. Query is a greeting or simple request\n",
        "    4. User has already been asked for clarification but he choose not to based on Chat History.\n",
        "    5. He asks for services we offer or other things related to website or bot.\n",
        "    6. He explicitly just asks for an answer or 2-3 clarifying questions has been asked from him already based on Chat History.\n",
        "\n",
        "    **Query Examples:**\n",
        "    Ambiguous: \"What happens if I have a problem with railway staff?\"\n",
        "    → `True`\n",
        "\n",
        "    Clear: \"What are my rights if RPF detains me for ticketless travel?\"\n",
        "    → `False`\n",
        "\n",
        "    Ambiguous: \"What can I do if police refuse to help me?\"\n",
        "    → `True`\n",
        "\n",
        "    Clear: \"How do I file a complaint against police misconduct during detention?\"\n",
        "    → `False`\n",
        "\n",
        "    Direct ask: \"Just give me the details/information/answer\"\n",
        "    -> `False`\n",
        "\n",
        "    Ambigous: Based on Chat History , 2-3 clarifying questions has already been asked from the user\n",
        "    -> `False`\n",
        "\n",
        "    **User Query:** {query}\n",
        "\n",
        "    **Chat History:** {memory}\n",
        "    '''\n",
        "\n",
        "    template = ChatPromptTemplate.from_template(\n",
        "        template = prompt,\n",
        "        partial_variables = {\n",
        "            'memory': memory\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = get_llm().with_structured_output(TalkBack)\n",
        "\n",
        "    chain = template | llm\n",
        "    return chain.invoke({'query': query}).model_dump()['talkback']"
      ],
      "metadata": {
        "id": "mLV-ZZ5arizk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WyegOKu8kKYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}