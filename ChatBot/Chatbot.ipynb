{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx9X/vVtXf+p5oEPs5nBEw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atulkhiyani0909/NyayaSetu/blob/main/ChatBot/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "TvQjPnDAg68p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "LBi0SE2NInv9"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain-core langchain langchain-google-genai pinecone-client langchain-pinecone langchain-huggingface pydantic gradio transformers pymongo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "1XOfUpvMhl4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "jzZ95U33g-Pm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ['PINECONE_API_KEY']=userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "os.environ['MONGO_URL']=userdata.get('MONGO_URL')"
      ],
      "metadata": {
        "id": "FfFKk6q7iDcq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "f-Xvdh2MkHrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict\n",
        "\n",
        "# Core ML and RAG Libraries\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from pinecone import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from pydantic import BaseModel,Field\n",
        "from functools import lru_cache\n",
        "from langchain.load import dumps,loads\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime\n",
        "from typing import Literal\n",
        "# import uuid\n",
        "# import time"
      ],
      "metadata": {
        "id": "ThN-H1Wsib1b"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_memory():\n",
        "#     \"\"\"Create isolated memory for each session\"\"\"\n",
        "#     return {\n",
        "#         \"history\": [],  # List of (user_msg, bot_msg) tuples\n",
        "#         \"session_id\": str(uuid.uuid4()),  # Unique session identifier\n",
        "#         \"created_at\": time.time()\n",
        "#     }\n",
        "# Initialize memory (can be replaced with LangChain memory if needed)\n",
        "def create_memory():\n",
        "    return {\"history\": []}  # Simple dictionary-based memory"
      ],
      "metadata": {
        "id": "HlysJlkGjYmg"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(maxsize=1)\n",
        "def get_llm(temp=0):\n",
        "    return ChatGoogleGenerativeAI(model='gemini-2.0-flash',temperature=temp)\n",
        "\n",
        "llm = get_llm()\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class language_detector(BaseModel):\n",
        "    language: str = Field(..., description=\"Detected Language\")\n",
        "    translated: str = Field(..., description=\"Translated to English\")\n",
        "\n",
        "def query_to_english(query:str,memory) -> dict:\n",
        "    \"\"\"Detects the language of the input query and translates it to English.\"\"\"\n",
        "\n",
        "    lan_example = '''{\n",
        "        \"language\": \"Hindi\",\n",
        "        \"translated\": \"Hello, how are you?\"\n",
        "    }'''\n",
        "\n",
        "    # history_context = memory.get(\"history\", []) if isinstance(memory, dict) else []\n",
        "\n",
        "    prompt = \"\"\"Translate the following query to clear English while preserving its context and intent you may imporve its wordings for better understanding.\n",
        "    If the query is ambiguous, you can rephrase it, but do not change its original meaning utilize this Chat history to rewrite\n",
        "    this ambigous query : {memory}\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Also, detect the language of the query and store it in \"language\".\n",
        "\n",
        "    Output should strictly follow this format:\n",
        "    {example}\n",
        "    \"\"\"\n",
        "\n",
        "    llm3 = get_llm().with_structured_output(language_detector)\n",
        "\n",
        "    trans_template = ChatPromptTemplate.from_template(\n",
        "        template=prompt,\n",
        "        partial_variables={\n",
        "            'example': lan_example,\n",
        "            'memory': memory\n",
        "            }\n",
        "    )\n",
        "\n",
        "    trans_chain = trans_template | llm3  # No need for StrOutputParser since output is structured\n",
        "    return trans_chain.invoke({'query': query}).model_dump()  # Ensure structured dict output"
      ],
      "metadata": {
        "id": "NWUcVbuLkwkI"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TalkBack(BaseModel):\n",
        "    talkback: bool = Field(..., description=\"Talkback\")\n",
        "\n",
        "#This decide should it talkback or should go for direct retrieval which ia a little time consuming!\n",
        "def should_talkback(query: str,memory) -> dict:\n",
        "    \"\"\"Should talkback or not\"\"\"\n",
        "\n",
        "    # history_context = memory.get(\"history\", []) if isinstance(memory, dict) else []\n",
        "\n",
        "    prompt = '''\n",
        "As a legal assistant for NyayaSetu,utilize memory for deciding -- only return `True` if:\n",
        "    1. The query is impossible to answer without more details (e.g., \"What happens?\" without context)\n",
        "    2. Combines 3+ unrelated legal issues\n",
        "    3. Contains contradictory information\n",
        "    4. Contains greeting or useless information or talk (cross check using chat_history)\n",
        "\n",
        "    Return `False` if:\n",
        "    1. Query describes a single clear legal issue\n",
        "    2. User has provided sufficient context\n",
        "    3. Query is a greeting or simple request\n",
        "    4. User has already been asked for clarification but he choose not to based on Chat History.\n",
        "    5. He asks for services we offer or other things related to website or bot.\n",
        "    6. He explicitly just asks for an answer or 2-3 clarifying questions has been asked from him already based on Chat History.\n",
        "\n",
        "    **Query Examples:**\n",
        "    Ambiguous: \"What happens if I have a problem with railway staff?\"\n",
        "    → `True`\n",
        "\n",
        "    Clear: \"What are my rights if RPF detains me for ticketless travel?\"\n",
        "    → `False`\n",
        "\n",
        "    Ambiguous: \"What can I do if police refuse to help me?\"\n",
        "    → `True`\n",
        "\n",
        "    Clear: \"How do I file a complaint against police misconduct during detention?\"\n",
        "    → `False`\n",
        "\n",
        "    Direct ask: \"Just give me the details/information/answer\"\n",
        "    -> `False`\n",
        "\n",
        "    Ambigous: Based on Chat History , 2-3 clarifying questions has already been asked from the user\n",
        "    -> `False`\n",
        "\n",
        "    **User Query:** {query}\n",
        "\n",
        "    **Chat History:** {memory}\n",
        "    '''\n",
        "\n",
        "    template = ChatPromptTemplate.from_template(\n",
        "        template = prompt,\n",
        "        partial_variables = {\n",
        "            'memory': memory\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = get_llm().with_structured_output(TalkBack)\n",
        "\n",
        "    chain = template | llm\n",
        "    return chain.invoke({'query': query}).model_dump()['talkback']"
      ],
      "metadata": {
        "id": "mLV-ZZ5arizk"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Talkback message"
      ],
      "metadata": {
        "id": "8dR2GEgutRzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def talkback(query: str,memory,language: str) -> str:\n",
        "\n",
        "    # history_context = memory.get(\"history\", []) if isinstance(memory, dict) else []\n",
        "\n",
        "    prompt = '''\n",
        "    You are NyaySetu, an AI-powered legal assistant developed by Team Normally Distributed. Your goal is to refine vague user queries by asking for more details to provide accurate legal guidance.\n",
        "\n",
        "    Also if the user require a normal response then provide it like answer to - Hi , morning etc.\n",
        "\n",
        "    ## Context:\n",
        "    - The user query may lack details, making it difficult to provide precise legal advice.\n",
        "    - Use the chat history to understand the context and determine what information has already been provided.\n",
        "    - Your task is to ask a single, logical follow-up question to clarify the user's intent or gather missing details.\n",
        "    - Keep the follow-up question concise, polite, and relevant to the query.\n",
        "    - You may ask multiple things or details in the same question so as to not irritate the user again and again.\n",
        "\n",
        "    ## Chat History:\n",
        "    {chat_history}\n",
        "\n",
        "    ## User Query:\n",
        "    {query}\n",
        "\n",
        "    ## Response Format:\n",
        "    - Reply in language as specified by the user in chat history but also see latest need (if available),secondary answer in {language} otherwise default to English.\n",
        "    - Provide only one follow-up question that helps clarify the query or gather additional details. -> can include multiple questions if needed.\n",
        "    - Ensure the response feels conversational and engaging.\n",
        "\n",
        "    Reply with only the follow-up question, nothing else.\n",
        "    '''\n",
        "\n",
        "    template = ChatPromptTemplate.from_template(\n",
        "        template = prompt,\n",
        "        partial_variables = {\n",
        "            'chat_history':memory,\n",
        "            'language':language\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = get_llm(0.5)\n",
        "\n",
        "    chain = template | llm | StrOutputParser()\n",
        "    return chain.invoke({'query': query})"
      ],
      "metadata": {
        "id": "WyegOKu8kKYe"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "rDEyUP3YCQnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever"
      ],
      "metadata": {
        "id": "A_tNC3rwCeGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'nyayasetu'\n",
        "pc = Pinecone()\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"BAAI/bge-base-en-v1.5\", #because our docs are written in english language that is why we used an specilised embedding model\n",
        "        model_kwargs={'device': 'cpu'},\n",
        "        encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index,embedding=embeddings)\n",
        "#Creating a retriever\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type = 'similarity_score_threshold',\n",
        "    search_kwargs = {'k':3,'score_threshold':0.7},\n",
        ")"
      ],
      "metadata": {
        "id": "6ZVIyTWOwCLl"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unique_union(documents:list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(docs) for sublist in documents for docs in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    #Return\n",
        "    return [loads(doc) for doc in unique_docs]"
      ],
      "metadata": {
        "id": "kQDnE5IADOCW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "o-fix4G5EKVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = MongoClient(os.getenv(\"MONGO_URL\"))\n",
        "db = client['nayaSetu']\n",
        "db.list_collection_names()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjVLcaoNdsz1",
        "outputId": "b5f946a0-b010-4fed-8b56-6902dc489e7d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['calls', 'workers', 'tickets', 'users', 'lawyers', 'admins', 'chatstats']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CounterDB:\n",
        "    def __init__(self):\n",
        "        self.client = MongoClient(os.getenv(\"MONGO_URL\"))\n",
        "        self.db = self.client.nayaSetu\n",
        "        self.counters = self.db.chatstats\n",
        "\n",
        "        # Create compound index matching your schema\n",
        "        self.counters.create_index(\n",
        "            [(\"department\", 1), (\"category\", 1)],\n",
        "            unique=True,\n",
        "            name=\"dept_cat_index\"\n",
        "        )\n",
        "\n",
        "    def update_counter(self, department: str, category: str):\n",
        "        \"\"\"Update counter and dates array per your schema\"\"\"\n",
        "        self.counters.update_one(\n",
        "            {\"department\": department, \"category\": category},\n",
        "            {\n",
        "                \"$inc\": {\"counter\": 1},\n",
        "                \"$push\": {\"dates\": datetime.utcnow()}\n",
        "            },\n",
        "            upsert=True\n",
        "        )"
      ],
      "metadata": {
        "id": "gL8c-eLzQDJD"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeptCategory(BaseModel):\n",
        "    department: Literal[\n",
        "        \"Municipal Services\",\n",
        "        \"Public Infrastructure\",\n",
        "        \"Healthcare\",\n",
        "        \"Education\",\n",
        "        \"Transportation\",\n",
        "        \"Public Utilities\",\n",
        "        \"Law Enforcement\",\n",
        "        \"Other\"\n",
        "    ] = Field(..., description=\"Primary department\")\n",
        "\n",
        "    category: Literal[\n",
        "        \"Corruption\",\n",
        "        \"Misconduct\",\n",
        "        \"Service Delays\",\n",
        "        \"Infrastructure Issues\",\n",
        "        \"Discrimination\",\n",
        "        \"Other\"\n",
        "    ] = Field(..., description=\"Issue category\")\n",
        "\n",
        "def extract_dept_category(query: str) -> dict:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Classify this Indian government complaint:\\n\\n\"\n",
        "        \"**Departments (EXACT MATCH REQUIRED)**:\\n\"\n",
        "        \"- Municipal Services: Local governance, sanitation, property tax\\n\"\n",
        "        \"- Public Infrastructure: Roads, bridges, public buildings\\n\"\n",
        "        \"- Healthcare: Hospitals, clinics, medical services\\n\"\n",
        "        \"- Education: Schools, colleges, educational policies\\n\"\n",
        "        \"- Transportation: Buses, trains, traffic management\\n\"\n",
        "        \"- Public Utilities: Electricity, water, gas services\\n\"\n",
        "        \"- Law Enforcement: Police, FIRs, investigations\\n\"\n",
        "        \"- Other: Unclassifiable or multi-department issues\\n\\n\"\n",
        "        \"**Categories (EXACT MATCH REQUIRED)**:\\n\"\n",
        "        \"- Corruption: Bribery, financial misconduct\\n\"\n",
        "        \"- Misconduct: Abuse of power, harassment\\n\"\n",
        "        \"- Service Delays: Late services, bureaucratic delays\\n\"\n",
        "        \"- Infrastructure Issues: Poor maintenance, construction defects\\n\"\n",
        "        \"- Discrimination: Unfair treatment based on identity\\n\"\n",
        "        \"- Other: Unclassifiable issues\\n\\n\"\n",
        "        \"**RULES**:\\n\"\n",
        "        \"1. Choose ONE department and ONE category\\n\"\n",
        "        \"2. Use EXACT names from lists above\\n\"\n",
        "        \"3. Default to 'Other' if uncertain\\n\\n\"\n",
        "        \"Complaint: \\\"{query}\\\"\\n\\n\"\n",
        "        \"Output ONLY JSON with keys 'department' and 'category'\"\n",
        "    )\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        temperature=0.1,\n",
        "        convert_system_message_to_human=True\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm.with_structured_output(DeptCategory)\n",
        "    return chain.invoke({\"query\": query}).model_dump()\n"
      ],
      "metadata": {
        "id": "pCQPc186e3w8"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(query: str,memory,language:str):\n",
        "\n",
        "    # Step 1: Extract department/category\n",
        "    try:\n",
        "        dept_cat = extract_dept_category(query)\n",
        "    except Exception as e:\n",
        "        print(f\"Extraction failed: {e}\")\n",
        "        dept_cat = {\"department\": \"other\", \"category\": \"other\"}\n",
        "\n",
        "    # Step 2: Update MongoDB counters - USING YOUR SCHEMA\n",
        "    db = CounterDB()\n",
        "    db.update_counter(dept_cat[\"department\"], dept_cat[\"category\"])\n",
        "\n",
        "    query_prompt = '''\n",
        "    You are a legal query optimization assistant.\n",
        "     Generate 5 distinct versions of the user’s question to improve retrieval of relevant legal documents from a vector database.\n",
        "\n",
        "    1. Make the 3 query generalised and rest 2 more concise.\n",
        "    2. Try to inlcude variations of how it can be asked by user or people.\n",
        "\n",
        "    Provide only the 5 rewritten questions separated by ',' i.e. a comma and do not include even a extra word other than these, this format is a must\n",
        "\n",
        "    Output format : query1,query2,query3,query4,query5\n",
        "\n",
        "    **Original Question**: {query}\n",
        "    '''\n",
        "\n",
        "    template = ChatPromptTemplate.from_template(\n",
        "        template = query_prompt\n",
        "    )\n",
        "\n",
        "    generate_queries = (\n",
        "        template\n",
        "        | get_llm(0.3)\n",
        "        | StrOutputParser()\n",
        "        | (lambda x: x.split(','))\n",
        "    )\n",
        "\n",
        "    # Retrieve Docs & Merge → Use rewritten queries to get unique union of documents.\n",
        "    retriever_chain = (\n",
        "        generate_queries\n",
        "        | retriever.map()\n",
        "        | (lambda x:get_unique_union(x))\n",
        "    )\n",
        "\n",
        "    rag_docs_list = retriever_chain.invoke(query)\n",
        "    rag_docs = '\\n'.join(str(doc) for doc in rag_docs_list) if rag_docs_list else \"No relevant RAG documents found.\"\n",
        "\n",
        "    prompt = '''\n",
        "You are NyayaSetu, one the best AI-powered legal assistant ,developed by Team Normally Distributed. Your primary role is to guide users through legal queries, government service issues, and complaint processes in India. You use advanced retrieval (RAG) technology to provide accurate, context-aware responses and always act with empathy and clarity.\n",
        "\n",
        "**Services you offer:**\n",
        "- **Complaint Management:** Help users submit and track complaints related to municipal services, public infrastructure, healthcare, education, transportation, public utilities, law enforcement, and more.\n",
        "  - When a user files a complaint, you guide them to select the relevant category, fill in details (subject, description, location, evidence etc.), and submit the form.\n",
        "  - Once submitted, the application is routed to the appropriate government authority for resolution.\n",
        "  - The user will be notified of the outcome: if resolved, the process ends; if rejected, the user can either escalate the complaint to a higher authority or directly connect with a lawyer via your platform.\n",
        "- **Legal Assistant:** Connect users with legal professionals for personalized advice. There are two options:\n",
        "  - **Instant Connect:** For urgent matters, users can immediately speak to an available lawyer.\n",
        "  - **Relevant Match:** Users can choose to be matched with a lawyer who specializes in the specific area of law related to their issue.\n",
        "- **Support Chatbot:** Provide instant, AI-powered guidance for general legal and government service questions.\n",
        "- **Document Verification & Other Services:** Inform users that these services are not currently available, but keep them updated about future offerings.They can see the services page on website\n",
        "\n",
        "**How you respond:**\n",
        "- Always suggest the most relevant service based on the user’s query, especially for complaints in the listed categories suggest that they can replort via our website along with the retrieved result include both but prompt our website a little more as convient and user friendly.\n",
        "- For legal or procedural questions, use your retrieval system to provide authoritative information and clear guidance.\n",
        "- If a user’s request cannot be addressed by your services or available data, politely inform them of the limitation and encourage them to seek help from the appropriate authority. Never provide incorrect or misleading information.\n",
        "\n",
        "**Tone and approach:**\n",
        "- Be supportive, professional, and concise.\n",
        "- Guide users step-by-step through processes, ensuring they understand their options at every stage.\n",
        "- Emphasize that your goal is to make legal and government services accessible and understandable for everyone.\n",
        "\n",
        "Always act as a knowledgeable, ethical, and user-focused legal gateway.\n",
        "\n",
        "    ## **User Query:**\n",
        "    {query}\n",
        "\n",
        "    ## **Context Information:**\n",
        "    You have access to three types of information:\n",
        "\n",
        "    1. **Conversation Memory**:\n",
        "    - Summary of the conversation till now between you and the user.\n",
        "    - Includes user preferences (language, tone, type of answers), major problems, and any prior responses.\n",
        "    - Avoid repeating answers unless explicitly requested by the user.\n",
        "\n",
        "    2. **Relevant Legal Documents (RAG Context)**:\n",
        "    - These are snippets from legal documents , Use them to provide accurate legal references or explanations.\n",
        "\n",
        "    3. **User Preferences**:\n",
        "    - Language preference specified by the user.\n",
        "    - Secondary use {language}\n",
        "    - Default language is English if no preference is clear.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### **Data Retrieved:**\n",
        "    **Conversation History:**\n",
        "    {memory}\n",
        "\n",
        "    **RAG Context (Relevant Legal Documents):**\n",
        "    {rag_docs}\n",
        "\n",
        "    ---\n",
        "    Now, based on the above context, generate the best possible response for the user.\n",
        "    ---\n",
        "    '''\n",
        "\n",
        "    rag_template = ChatPromptTemplate.from_template(\n",
        "        template = prompt,\n",
        "        partial_variables = {\n",
        "            'memory':memory,\n",
        "            'rag_docs':rag_docs,\n",
        "            'language':language\n",
        "        }\n",
        "    )\n",
        "\n",
        "    rag_chain = rag_template | get_llm(0.5) | StrOutputParser()\n",
        "    return rag_chain.invoke({'query': query})"
      ],
      "metadata": {
        "id": "OTDk5HAeD0nx"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(query:str,memory:list,language:str):\n",
        "    intent = should_talkback(query,memory)\n",
        "    if intent == True:\n",
        "        return talkback(query,memory,language)\n",
        "    else:\n",
        "        return rag(query,memory,language)"
      ],
      "metadata": {
        "id": "CU4-rS4ITZUY"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chatbot_interface(user_input, chat_history, session_state):\n",
        "    if session_state.get(\"memory\") is None:\n",
        "        session_state[\"memory\"] = create_memory()\n",
        "\n",
        "    # Translate query and detect language\n",
        "    trans_dict = query_to_english(user_input, session_state[\"memory\"])\n",
        "    eng_query = trans_dict[\"translated\"]\n",
        "    language = trans_dict[\"language\"]\n",
        "\n",
        "    # Get response\n",
        "    response = chatbot_response(eng_query, session_state[\"memory\"], language)\n",
        "\n",
        "    # Store conversation history\n",
        "    session_state[\"memory\"][\"history\"].append((user_input, response))\n",
        "\n",
        "    return \"\", chat_history + [(user_input, response)], session_state\n",
        "\n",
        "def reset_chat_():\n",
        "    return [], {\"memory\": None}\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    session_state = gr.State(value={\"memory\": None})\n",
        "    chatbot = gr.Chatbot([])\n",
        "    user_input = gr.Textbox(placeholder=\"Ask something...\")\n",
        "    submit_btn = gr.Button(\"Send\")\n",
        "    reset_btn = gr.Button(\"Reset\")\n",
        "\n",
        "    outputs = [user_input, chatbot, session_state]\n",
        "\n",
        "    user_input.submit(chatbot_interface, [user_input, chatbot, session_state], outputs)\n",
        "    submit_btn.click(chatbot_interface, [user_input, chatbot, session_state], outputs)\n",
        "    reset_btn.click(reset_chat_, [], [chatbot, session_state])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "QgUN4BCNUe-f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "cd546d75-4e23-422a-92b5-9821a621ba2d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-71-1164648138.py:25: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot([])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://717f1e2f39be786187.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://717f1e2f39be786187.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OWBE9zcIW_VX"
      },
      "execution_count": 71,
      "outputs": []
    }
  ]
}