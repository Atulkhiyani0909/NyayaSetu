{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUXlPwlSAANgCXgiOHup2x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atulkhiyani0909/NyayaSetu/blob/main/ChatBot/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "TvQjPnDAg68p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LBi0SE2NInv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6c0ec9-e742-4539-8b8d-ce659de26429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.3/524.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain-core langchain langchain-google-genai pinecone-client langchain-pinecone langchain-huggingface pydantic gradio transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "1XOfUpvMhl4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "jzZ95U33g-Pm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ['PINECONE_API_KEY']=userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "FfFKk6q7iDcq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "f-Xvdh2MkHrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict\n",
        "\n",
        "# Core ML and RAG Libraries\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from pinecone import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from pydantic import BaseModel,Field\n",
        "from functools import lru_cache\n",
        "from langchain.load import dumps,loads"
      ],
      "metadata": {
        "id": "ThN-H1Wsib1b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(maxsize=1)\n",
        "def get_llm(temp=0):\n",
        "    return ChatGoogleGenerativeAI(model='gemini-2.0-flash',temperature=temp)\n",
        "\n",
        "llm = get_llm()\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class language_detector(BaseModel):\n",
        "    language: str = Field(..., description=\"Detected Language\")\n",
        "    translated: str = Field(..., description=\"Translated to English\")\n",
        "\n",
        "def query_to_english(query:str,memory) -> dict:\n",
        "    \"\"\"Detects the language of the input query and translates it to English.\"\"\"\n",
        "\n",
        "    lan_example = '''{\n",
        "        \"language\": \"Hindi\",\n",
        "        \"translated\": \"Hello, how are you?\"\n",
        "    }'''\n",
        "\n",
        "    prompt = \"\"\"Translate the following query to clear English while preserving its context and intent you may imporve its wordings for better understanding.\n",
        "    If the query is ambiguous, you can rephrase it, but do not change its original meaning utilize this Chat history to rewrite\n",
        "    this ambigous query : {memory}\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Also, detect the language of the query and store it in \"language\".\n",
        "\n",
        "    Output should strictly follow this format:\n",
        "    {example}\n",
        "    \"\"\"\n",
        "\n",
        "    llm3 = get_llm().with_structured_output(language_detector)\n",
        "\n",
        "    trans_template = ChatPromptTemplate.from_template(\n",
        "        template=prompt,\n",
        "        partial_variables={\n",
        "            'example': lan_example,\n",
        "            'memory': memory\n",
        "            }\n",
        "    )\n",
        "\n",
        "    trans_chain = trans_template | llm3  # No need for StrOutputParser since output is structured\n",
        "    return trans_chain.invoke({'query': query}).model_dump()  # Ensure structured dict output"
      ],
      "metadata": {
        "id": "NWUcVbuLkwkI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TalkBack(BaseModel):\n",
        "    talkback: bool = Field(..., description=\"Talkback\")\n",
        "\n",
        "#This decide should it talkback or should go for direct retrieval which ia a little time consuming!\n",
        "def should_talkback(query: str,memory) -> dict:\n",
        "    \"\"\"Should talkback or not\"\"\"\n",
        "\n",
        "    prompt = '''\n",
        "As a legal assistant for NyayaSetu,utilize memory for deciding -- only return `True` if:\n",
        "    1. The query is impossible to answer without more details (e.g., \"What happens?\" without context)\n",
        "    2. Combines 3+ unrelated legal issues\n",
        "    3. Contains contradictory information\n",
        "    4. Contains greeting or useless information or talk (cross check using chat_history)\n",
        "\n",
        "    Return `False` if:\n",
        "    1. Query describes a single clear legal issue\n",
        "    2. User has provided sufficient context\n",
        "    3. Query is a greeting or simple request\n",
        "    4. User has already been asked for clarification but he choose not to based on Chat History.\n",
        "    5. He asks for services we offer or other things related to website or bot.\n",
        "    6. He explicitly just asks for an answer or 2-3 clarifying questions has been asked from him already based on Chat History.\n",
        "\n",
        "    **Query Examples:**\n",
        "    Ambiguous: \"What happens if I have a problem with railway staff?\"\n",
        "    → `True`\n",
        "\n",
        "    Clear: \"What are my rights if RPF detains me for ticketless travel?\"\n",
        "    → `False`\n",
        "\n",
        "    Ambiguous: \"What can I do if police refuse to help me?\"\n",
        "    → `True`\n",
        "\n",
        "    Clear: \"How do I file a complaint against police misconduct during detention?\"\n",
        "    → `False`\n",
        "\n",
        "    Direct ask: \"Just give me the details/information/answer\"\n",
        "    -> `False`\n",
        "\n",
        "    Ambigous: Based on Chat History , 2-3 clarifying questions has already been asked from the user\n",
        "    -> `False`\n",
        "\n",
        "    **User Query:** {query}\n",
        "\n",
        "    **Chat History:** {memory}\n",
        "    '''\n",
        "\n",
        "    template = ChatPromptTemplate.from_template(\n",
        "        template = prompt,\n",
        "        partial_variables = {\n",
        "            'memory': memory\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = get_llm().with_structured_output(TalkBack)\n",
        "\n",
        "    chain = template | llm\n",
        "    return chain.invoke({'query': query}).model_dump()['talkback']"
      ],
      "metadata": {
        "id": "mLV-ZZ5arizk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Talkback message"
      ],
      "metadata": {
        "id": "8dR2GEgutRzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def talkback(query: str,memory,language: str) -> str:\n",
        "\n",
        "    prompt = '''\n",
        "    You are NyaySetu, an AI-powered legal assistant developed by Team Normally Distributed. Your goal is to refine vague user queries by asking for more details to provide accurate legal guidance.\n",
        "\n",
        "    Also if the user require a normal response then provide it like answer to - Hi , morning etc.\n",
        "\n",
        "    ## Context:\n",
        "    - The user query may lack details, making it difficult to provide precise legal advice.\n",
        "    - Use the chat history to understand the context and determine what information has already been provided.\n",
        "    - Your task is to ask a single, logical follow-up question to clarify the user's intent or gather missing details.\n",
        "    - Keep the follow-up question concise, polite, and relevant to the query.\n",
        "    - You may ask multiple things or details in the same question so as to not irritate the user again and again.\n",
        "\n",
        "    ## Chat History:\n",
        "    {chat_history}\n",
        "\n",
        "    ## User Query:\n",
        "    {query}\n",
        "\n",
        "    ## Response Format:\n",
        "    - Reply in language as specified by the user in chat history but also see latest need (if available),secondary answer in {language} otherwise default to English.\n",
        "    - Provide only one follow-up question that helps clarify the query or gather additional details. -> can include multiple questions if needed.\n",
        "    - Ensure the response feels conversational and engaging.\n",
        "\n",
        "    Reply with only the follow-up question, nothing else.\n",
        "    '''\n",
        "\n",
        "    template = ChatPromptTemplate.from_template(\n",
        "        template = prompt,\n",
        "        partial_variables = {\n",
        "            'chat_history':memory,\n",
        "            'language':language\n",
        "        }\n",
        "    )\n",
        "\n",
        "    llm = get_llm(0.5)\n",
        "\n",
        "    chain = template | llm | StrOutputParser()\n",
        "    return chain.invoke({'query': query})"
      ],
      "metadata": {
        "id": "WyegOKu8kKYe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "rDEyUP3YCQnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever"
      ],
      "metadata": {
        "id": "A_tNC3rwCeGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'nyayasetu'\n",
        "pc = Pinecone()\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"BAAI/bge-base-en-v1.5\", #because our docs are written in english language that is why we used an specilised embedding model\n",
        "        model_kwargs={'device': 'cpu'},\n",
        "        encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index,embedding=embeddings)\n",
        "#Creating a retriever\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type = 'similarity_score_threshold',\n",
        "    search_kwargs = {'k':3,'score_threshold':0.7},\n",
        ")"
      ],
      "metadata": {
        "id": "6ZVIyTWOwCLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unique_union(documents:list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(docs) for sublist in documents for docs in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    #Return\n",
        "    return [loads(doc) for doc in unique_docs]"
      ],
      "metadata": {
        "id": "kQDnE5IADOCW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "o-fix4G5EKVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(query: str,memory,language:str):\n",
        "\n",
        "    query_prompt = '''\n",
        "    You are a legal query optimization assistant.\n",
        "     Generate 5 distinct versions of the user’s question to improve retrieval of relevant legal documents from a vector database.\n",
        "\n",
        "    1. Make the 3 query generalised and rest 2 more concise.\n",
        "    2. Try to inlcude variations of how it can be asked by user or people.\n",
        "\n",
        "    Provide only the 5 rewritten questions separated by ',' i.e. a comma and do not include even a extra word other than these, this format is a must\n",
        "\n",
        "    Output format : query1,query2,query3,query4,query5\n",
        "\n",
        "    **Original Question**: {query}\n",
        "    '''\n",
        "\n",
        "    template = ChatPromptTemplate.from_template(\n",
        "        template = query_prompt\n",
        "    )\n",
        "\n",
        "    generate_queries = (\n",
        "        template\n",
        "        | get_llm(0.3)\n",
        "        | StrOutputParser()\n",
        "        | (lambda x: x.split(','))\n",
        "    )\n",
        "\n",
        "    # Retrieve Docs & Merge → Use rewritten queries to get unique union of documents.\n",
        "    retriever_chain = (\n",
        "        generate_queries\n",
        "        | retriever.map()\n",
        "        | (lambda x:get_unique_union(x))\n",
        "    )\n",
        "\n",
        "    rag_docs_list = retriever_chain.invoke(query)\n",
        "    rag_docs = '\\n'.join(str(doc) for doc in rag_docs_list) if rag_docs_list else \"No relevant RAG documents found.\"\n",
        "\n",
        "    prompt = '''\n",
        "You are NyayaSetu, one the best AI-powered legal assistant ,developed by Team Normally Distributed. Your primary role is to guide users through legal queries, government service issues, and complaint processes in India. You use advanced retrieval (RAG) technology to provide accurate, context-aware responses and always act with empathy and clarity.\n",
        "\n",
        "**Services you offer:**\n",
        "- **Complaint Management:** Help users submit and track complaints related to municipal services, public infrastructure, healthcare, education, transportation, public utilities, law enforcement, and more.\n",
        "  - When a user files a complaint, you guide them to select the relevant category, fill in details (subject, description, location, evidence etc.), and submit the form.\n",
        "  - Once submitted, the application is routed to the appropriate government authority for resolution.\n",
        "  - The user will be notified of the outcome: if resolved, the process ends; if rejected, the user can either escalate the complaint to a higher authority or directly connect with a lawyer via your platform.\n",
        "- **Legal Assistant:** Connect users with legal professionals for personalized advice. There are two options:\n",
        "  - **Instant Connect:** For urgent matters, users can immediately speak to an available lawyer.\n",
        "  - **Relevant Match:** Users can choose to be matched with a lawyer who specializes in the specific area of law related to their issue.\n",
        "- **Support Chatbot:** Provide instant, AI-powered guidance for general legal and government service questions.\n",
        "- **Document Verification & Other Services:** Inform users that these services are not currently available, but keep them updated about future offerings.They can see the services page on website\n",
        "\n",
        "**How you respond:**\n",
        "- Always suggest the most relevant service based on the user’s query, especially for complaints in the listed categories suggest that they can replort via our website along with the retrieved result include both but prompt our website a little more as convient and user friendly.\n",
        "- For legal or procedural questions, use your retrieval system to provide authoritative information and clear guidance.\n",
        "- If a user’s request cannot be addressed by your services or available data, politely inform them of the limitation and encourage them to seek help from the appropriate authority. Never provide incorrect or misleading information.\n",
        "\n",
        "**Tone and approach:**\n",
        "- Be supportive, professional, and concise.\n",
        "- Guide users step-by-step through processes, ensuring they understand their options at every stage.\n",
        "- Emphasize that your goal is to make legal and government services accessible and understandable for everyone.\n",
        "- Use Hindi or regional phrases where appropriate to build trust and connection (e.g., \"जहां न्याय की आस हो, NyaySetu आपके साथ है।\").\n",
        "\n",
        "Always act as a knowledgeable, ethical, and user-focused legal gateway.\n",
        "\n",
        "    ## **User Query:**\n",
        "    {query}\n",
        "\n",
        "    ## **Context Information:**\n",
        "    You have access to three types of information:\n",
        "\n",
        "    1. **Conversation Memory**:\n",
        "    - Summary of the conversation till now between you and the user.\n",
        "    - Includes user preferences (language, tone, type of answers), major problems, and any prior responses.\n",
        "    - Avoid repeating answers unless explicitly requested by the user.\n",
        "\n",
        "    2. **Relevant Legal Documents (RAG Context)**:\n",
        "    - These are snippets from legal documents , Use them to provide accurate legal references or explanations.\n",
        "\n",
        "    3. **User Preferences**:\n",
        "    - Language preference specified by the user.\n",
        "    - Secondary use {language}\n",
        "    - Default language is English if no preference is clear.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### **Data Retrieved:**\n",
        "    **Conversation History:**\n",
        "    {memory}\n",
        "\n",
        "    **RAG Context (Relevant Legal Documents):**\n",
        "    {rag_docs}\n",
        "\n",
        "    ---\n",
        "    Now, based on the above context, generate the best possible response for the user.\n",
        "    ---\n",
        "    '''\n",
        "\n",
        "    rag_template = ChatPromptTemplate.from_template(\n",
        "        template = prompt,\n",
        "        partial_variables = {\n",
        "            'memory':memory,\n",
        "            'rag_docs':rag_docs,\n",
        "            'language':language\n",
        "        }\n",
        "    )\n",
        "\n",
        "    rag_chain = rag_template | get_llm(0.5) | StrOutputParser()\n",
        "    return rag_chain.invoke({'query': query})"
      ],
      "metadata": {
        "id": "OTDk5HAeD0nx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(query:str,memory:list,language:str):\n",
        "    intent = should_talkback(query,memory)\n",
        "    if intent == True:\n",
        "        return talkback(query,memory,language)\n",
        "    else:\n",
        "        return rag(query,memory,language)"
      ],
      "metadata": {
        "id": "CU4-rS4ITZUY"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}